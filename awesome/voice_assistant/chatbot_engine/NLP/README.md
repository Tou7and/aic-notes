# Nature Language Processing

# [HuggingFace](https://huggingface.co/)
- Build, train and deploy SOTA models powered by the reference open source in machine learning.

# [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- Official Github: [google-research/bert](https://github.com/google-research/bert)
- HuggingFace Model Hubs: [bert-base-uncased](https://huggingface.co/bert-base-uncased)

> BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. 
> It is pretrained on the raw texts only without human annotations.

Pretrained with two objectives:
1. Masked language modeling (MLM)
2. Next sentence prediction (NSP)


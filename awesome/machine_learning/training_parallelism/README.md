# Training Parallelism
For training large neural networks, training parallelism is necessary.

# Blogs
[How to Train Really Large Models on Many GPUs](https://lilianweng.github.io/lil-log/2021/09/24/train-large-neural-networks.html)
- Data Parallelism
- Model Parallelism
- Pipeline Parallelism
- Tensor Parallelism
- Mixture-of-Experts (MoE)

# Tools
[Deep Speed](https://www.deepspeed.ai/)
- Microsoft DeepSpeed project
- pip install deepspeed
- [Tutorials](https://www.deepspeed.ai/tutorials/)


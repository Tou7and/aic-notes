# Deep Learning

# Sequence to sequence
[Attention is all you need](https://arxiv.org/abs/1706.03762)
- [Transformer Network Explained](https://www.youtube.com/watch?v=TQQlZhbC5ps)
- Transformer: Encoder + Decoder
  - Encoder: attention + feed forward networks
  - Decoder: attention + feed forward networks

Transformer Family?
- Encoder only Transformer
- Decoder only Transformer
- Conformer
- Reformer


